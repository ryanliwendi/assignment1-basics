model:
  vocab_size: 10000
  context_length: 256
  num_layers: 4
  num_heads: 16
  d_model: 512
  d_ff: 1344
  theta: 10000.0

optim:
  betas: [0.9, 0.999]
  eps: 0.00000001
  weight_decay: 0.1

lr_schedule:
  alpha_max: 0.001
  alpha_min: 0.00001
  t_warm: 100
  t_cos: 10000

training:
  batch_size: 128
  max_steps: 10000
  max_norm: 1.0
  device: mps

data:
  train_path: outputs/tinystories/train_encoded.npy
  val_path: outputs/tinystories/valid_encoded.npy

logging:
  log_interval: 10
  eval_interval: 500
  eval_batches: 50

checkpoints:
  checkpoint_dir: checkpoints/
  checkpoint_interval: 500