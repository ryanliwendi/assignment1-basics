model:
  vocab_size: 32000
  context_length: 256
  num_layers: 4
  num_heads: 16
  d_model: 512
  d_ff: 1344
  theta: 10000.0

optim:
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.1

lr_schedule:
  alpha_max: 1e-3
  alpha_min: 1e-5
  t_warm: 100
  t_cos: 10000

training:
  batch_size: 128
  max_steps: 10000
  max_norm: 1.0
  device: cuda

data:
  train_path: outputs/openwebtext/train_encoded.npy
  val_path: outputs/openwebtext/valid_encoded.npy

logging:
  log_interval: 10
  eval_interval: 500
  eval_batches: 50

checkpoints:
  checkpoint_dir: checkpoints/
  checkpoint_interval: 500